{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step-by-step LSTM implementation using the processes described in [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). Note that the diagrams in this notebook are referenced from this resource, and were created by [Christopher Olah](https://colah.github.io/about.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the activation functions used in the unit\n",
    "\n",
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x: np.ndarray) -> np.ndarray:\n",
    "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Forget Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Forget Gate](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.13803426, -1.93312646,  0.12416734, -2.44108129,  0.52594301,\n",
       "        0.97341724, -0.68083653,  1.12971513,  0.27286152, -1.18094133,\n",
       "        1.15826649,  0.21414934,  0.9412887 ,  0.73235329,  1.03968119,\n",
       "        0.97071932, -0.3471485 ,  1.72729094,  0.37225874, -0.21539921,\n",
       "        0.16484434, -0.93286577,  0.32170747, -0.69557243, -1.23316597,\n",
       "       -0.39036439, -2.11648016,  1.17079354,  1.16807481,  1.81068018,\n",
       "        0.95454614,  0.63671112])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 32\n",
    "\n",
    "# Input to unit\n",
    "x = np.random.normal(loc=0, scale=1, size=input_size)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_size = 64\n",
    "\n",
    "# Hidden state initialization\n",
    "h = np.zeros(shape=state_size)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell state initialisation\n",
    "c = np.zeros(shape=state_size)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.13803426,\n",
       "       -1.93312646,  0.12416734, -2.44108129,  0.52594301,  0.97341724,\n",
       "       -0.68083653,  1.12971513,  0.27286152, -1.18094133,  1.15826649,\n",
       "        0.21414934,  0.9412887 ,  0.73235329,  1.03968119,  0.97071932,\n",
       "       -0.3471485 ,  1.72729094,  0.37225874, -0.21539921,  0.16484434,\n",
       "       -0.93286577,  0.32170747, -0.69557243, -1.23316597, -0.39036439,\n",
       "       -2.11648016,  1.17079354,  1.16807481,  1.81068018,  0.95454614,\n",
       "        0.63671112])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the hidden state and the input\n",
    "xh = np.concatenate([h, x], axis=0)\n",
    "xh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 96)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a weight matrix for the forget gate \n",
    "wf = np.random.normal(loc=0, scale=1, size=(state_size, input_size + state_size))\n",
    "wf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a bias vector for the forget gate\n",
    "bf = np.random.normal(loc=0, scale=1, size=state_size)\n",
    "bf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the output of the forget gate (this will be used to determine how much of the cell state will be remembered)\n",
    "# Note, np.matmul performs a matrix multiplication - not element-wise. This brings ft to the same size as the hidden state\n",
    "ft = sigmoid(np.matmul(wf, xh) + bf)\n",
    "ft.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Input Gate](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 96)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a weight matrix for the input gate \n",
    "wi = np.random.normal(loc=0, scale=1, size=(state_size, input_size + state_size))\n",
    "wi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a bias vector for the input gate\n",
    "bi = np.random.normal(loc=0, scale=1, size=state_size)\n",
    "bi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine how much of the candidate memory will be added to the cell state\n",
    "it = sigmoid(np.matmul(wi, xh) + bi)\n",
    "it.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 96)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a weight matrix for the candidate memory\n",
    "wc = np.random.normal(loc=0, scale=1, size=(state_size, input_size + state_size))\n",
    "wc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a bias vector for the candidate memory\n",
    "bc = np.random.normal(loc=0, scale=1, size=state_size)\n",
    "bc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a candidate memory - that together with the input gate will determine how much of the new memory will be added to the cell state\n",
    "ct = tanh(np.matmul(wc, xh) + bc)\n",
    "ct.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the cell state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Input Gate](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the forget date: this determines how much information from c to be retained. At t=0, c is already 0, so it stays 0\n",
    "# Note, np.multiply performs a Hadamard product - elementwise multiplication\n",
    "c = np.multiply(c, ft)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.38800982e-01, -7.53439349e-04,  3.58659969e-01,  9.99235875e-01,\n",
       "        9.99999265e-01, -7.61019182e-01,  3.01116537e-01, -5.49924149e-07,\n",
       "       -3.16378501e-01,  2.83966638e-02,  9.53157079e-01, -8.93158357e-01,\n",
       "       -7.92177449e-01, -8.04863664e-01,  9.98107341e-01, -2.42649259e-05,\n",
       "       -1.35533654e-02,  3.09467352e-02,  1.94439177e-06,  9.20814307e-01,\n",
       "       -1.47979049e-02, -1.84869987e-03, -2.00609528e-01, -7.85074701e-01,\n",
       "       -1.32092107e-04, -9.80962061e-01,  7.98297118e-01,  9.15804030e-01,\n",
       "       -3.31050278e-04, -9.36913004e-01,  9.93993163e-01, -8.59933977e-01,\n",
       "        6.56381509e-02, -3.89763543e-01, -2.76786670e-04,  9.43913444e-01,\n",
       "       -1.80635729e-03, -9.71571248e-01, -9.10251911e-01, -2.45127430e-05,\n",
       "        7.32519813e-03,  9.73431090e-01, -5.83415417e-01,  1.88446187e-02,\n",
       "       -8.97685792e-02, -7.37082690e-01,  9.94785868e-01, -9.52868540e-01,\n",
       "        2.93860591e-04, -1.46519553e-07,  9.71011989e-01, -9.37907115e-01,\n",
       "       -8.76424878e-01,  9.42900239e-01,  1.69039460e-01,  7.58656175e-01,\n",
       "       -1.88288423e-07, -1.00591559e-02,  2.65759346e-03, -3.29684953e-04,\n",
       "        9.97565570e-01, -1.10624975e-01,  3.23397034e-02,  1.11137505e-01])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the candidate memory and input date - how much of a new memory is added to the cell state\n",
    "c = c + np.multiply(it, ct)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Output Gate](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 96)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a weight matrix for the output gate \n",
    "wo = np.random.normal(loc=0, scale=1, size=(state_size, input_size + state_size))\n",
    "wo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a bias vector for the output gate\n",
    "bo = np.random.normal(loc=0, scale=1, size=state_size)\n",
    "bo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the output of the output gate (this will be used to determine how much of the candidate hidden state will be remembered)\n",
    "ot = sigmoid(np.matmul(wo, xh) + bo)\n",
    "ot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine how much of the candidate hidden state is remembered and will be output\n",
    "ht = np.multiply(ot, tanh(c))\n",
    "ht.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.79681526e-01, -1.54263601e-09,  3.43982834e-01,  1.46485487e-02,\n",
       "        7.59776185e-01, -1.34679082e-02,  2.71141059e-01, -1.77521974e-10,\n",
       "       -7.24204706e-05,  2.24342093e-04,  7.28472162e-01, -4.72137811e-01,\n",
       "       -6.24029027e-01, -6.47689143e-01,  4.20244364e-05, -2.41515575e-05,\n",
       "       -4.98548390e-04,  3.08196778e-02,  1.79760902e-06,  9.52174501e-02,\n",
       "       -9.62503962e-03, -2.13938142e-07, -6.34236340e-02, -6.55596083e-01,\n",
       "       -1.29694798e-04, -2.97955357e-03,  4.64368611e-05,  4.34340277e-02,\n",
       "       -8.39692889e-07, -4.93680747e-09,  7.49435172e-01, -2.22465684e-03,\n",
       "        3.38044892e-02, -1.73359790e-02, -2.64954201e-05,  7.37014819e-01,\n",
       "       -9.77417233e-09, -6.50363498e-01, -9.55189796e-02, -8.15535128e-06,\n",
       "        2.18111030e-03,  2.04701184e-02, -5.24989980e-01,  1.19277503e-02,\n",
       "       -1.05202698e-02, -6.27168648e-01,  1.15308414e-03, -1.09548866e-01,\n",
       "        2.89398260e-04, -2.26840959e-08,  5.16726511e-01, -4.18480636e-01,\n",
       "       -3.08249623e-04,  1.27585932e-01,  1.46455682e-03,  6.40177234e-01,\n",
       "       -1.71016961e-11, -1.38158327e-03,  2.42077528e-03, -3.92619886e-05,\n",
       "        7.60566990e-01, -1.10140534e-01,  3.23281781e-02,  8.83751062e-02])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
